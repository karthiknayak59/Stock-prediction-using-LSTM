# -*- coding: utf-8 -*-
"""Multi-stock multi-feature modeling v2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R4pcxlL9QxZE1BqYd9b_JyH8UBBhFHVP

# Setup and Data Collection
"""

import pandas as pd
import numpy as np
import math
#import matplotlib.pyplot as plt

import time
import os
import sys
import logging

import warnings
warnings.filterwarnings("ignore")
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # FATAL
logging.getLogger('tensorflow').setLevel(logging.INFO)

import tensorflow
#tensorflow.compat.v1.logging.set_verbosity(tensorflow.compat.v1.logging.ERROR)

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.layers import LSTM
from tensorflow.python.keras.layers import Dropout
from tensorflow.python.keras.layers import BatchNormalization
from tensorflow.python import keras

###########################################################
################# STREAMING TO CSV FILES ##################
###########################################################

def zero_pad(x, target=2):
	out = str(x)
	for i in range(0,target-len(str(x))):
		out = "0" + out
	return out

def valid_stock(symb,desired_stocks=[]):
	if desired_stocks == []: return symb.isalnum()
	return symb.isalnum() and symb in desired_stocks

def make_output_fn(symb, year, target_dir):
	return os.path.join(target_dir,year+'_'+symb+'.csv')

def getRange(time_data):
	if time_data.find(':') == -1 or time_data < '10:00':
		return '09:00'
	elif time_data < '11:00': return '10:00'
	elif time_data < '12:00': return '11:00'
	elif time_data < '13:00': return '12:00'
	elif time_data < '14:00': return '13:00'
	elif time_data < '15:00': return '14:00'
	else: return '15:00'
 
def retrieve_day(parent_dir, year, month, day, target_dir, desired_stocks=[]):
	# create filepath and retrieve streaming file
	fp = os.path.join(parent_dir,year,zero_pad(month),zero_pad(day),'streaming.tsv')
	day_tbl = pd.read_csv(fp,delim_whitespace=True)
	
	# drop designated columns
	day_tbl.drop(['ASK','BID','CHANGE','PCT_CH'],axis=1,inplace=True)
	# add date column
	day_tbl['DATE'] = year+'-'+month+'-'+day
	# replace time column with time range
	day_tbl.loc[:,'TIME'] = day_tbl['TIME'].apply(lambda x: getRange(str(x))) 
	
	# for each stock, append all lines to corresponding file
	for symb, grp in day_tbl.groupby('SYMB'):
		if valid_stock(symb,desired_stocks):
			# convert from running volume to volume per transaction
			temp = grp['VOLUME'].iloc[0]
			grp.loc[:,'VOLUME'] = grp['VOLUME'].diff(1)
			grp.loc[:,'VOLUME'].iloc[0] = temp
			# aggregate data based on date-time combination
			grp = grp.groupby(['DATE','TIME'] ).agg({
				'VOLUME': 'sum', 
				'PRICE': 'mean',
				'OPEN': 'mean',
				'HIGH':'max',
				'LOW':'min'
			    }).reset_index()
			# write/append to stock file
			output_path = make_output_fn(symb,year,target_dir)
			grp.to_csv(output_path,
			           mode = 'a' if os.path.exists(output_path) else 'w',
					   header = not os.path.exists(output_path),
					   encoding='utf-8',
					   index = False)

def retrieve_month(parent_dir, year, month, target_dir,desired_stocks=[]):
	#start = time.time()
	for i in range(1,31):
		fp = os.path.join(parent_dir,year,month,zero_pad(i))
		if os.path.isdir(fp):
			#start_day = time.time()
			#print('retrieving streaming data for day',i)
			retrieve_day(parent_dir,year,month,zero_pad(i),target_dir,desired_stocks)
			#print("Completed day %d in %.2f" % (i,time.time() - start_day) + " seconds")
	#print("Completed MONTH " + month + " in %.2f" % (time.time() - start) + " seconds")

def retrieve_months(parent_dir, year, months, target_dir,desired_stocks=[]):
	for month in months:
		retrieve_month(parent_dir,year,month,target_dir,desired_stocks)

###########################################################
############# PREPPING CSV FOR INPUT TO MODEL #############
###########################################################

# reformat date column then sort data by date (in-place)
def reformat_and_sort_by_date(df):
	df.sort_values(['DATE','TIME'],inplace=True)

# scale an entire df (dataframe) (in-place); returns scaler for 'PRICE' column
def scale_df_col(df):
	for colName, colData in df.iteritems():
		scaler = MinMaxScaler(feature_range=(0,1))
		df[colName] = scaler.fit_transform(df[[colName]])
		if colName == 'PRICE':
			scaler_out = scaler
	return scaler_out

# linear interpolation for blank values
def fill_blanks(df):
	if not np.all(np.isfinite(df.values)):
		#print("Empty/NaN values detected. Linearly interpolating values...")
		df.interpolate(method="linear",inplace=True)
		df.fillna(method="bfill",inplace=True)

def prep_data(df):
	# reformat and sort data by date
	reformat_and_sort_by_date(df)
	# save DATE and TIME columns prior to dropping
	data_dt = df[['DATE','TIME']].copy()
	# drop the DATE and TIME columns
	df.drop(['DATE','TIME'],axis=1,inplace=True)
	# get the column index of price
	price_col = df.columns.tolist().index('PRICE')
	# scale data
	scaler = scale_df_col(df)
	# fill NaN/blank values
	fill_blanks(df)
	return df, scaler, data_dt, price_col

# given a dataframe (df) with (rows,cols)=(L,w) and a timestep (ts)
# each input+output frame has dimensions (ts + 1, w)
# there are (L - (ts+1) + 1) frames
# NOTE: assumes that we are predicting on ONLY price
def make_input_data(df, ts=1):
	L = len(df.index)
	w = len(df.columns)
	# pre-allocate space for numpy arrays
	x = np.empty((L - ts, ts, w))
	y = np.empty((L - ts, 1,  1)) # 1 instead of w to predict on price
	for i in range(L-ts):
		x[i] = df.iloc[i:(i+ts),:]    # slice rows
		y[i] = df.iloc[i+ts]['PRICE'] # get the single cell value
	return x,y

###########################################################
#################### USING KERAS MODEL ####################
###########################################################

# alternative optimizer: 'rmsprop'
def create_model(i_shape,opt='adam'):
	model = Sequential()
	model.add(BatchNormalization(input_shape=i_shape))
	model.add(LSTM(50,return_sequences=True))
	model.add(Dropout(0.2))
	model.add(LSTM(50,return_sequences=True))
	model.add(Dropout(0.2))
	model.add(LSTM(50,return_sequences=True))
	model.add(Dropout(0.3))
	model.add(LSTM(50))
	model.add(Dropout(0.4))
	model.add(Dense(1))
	model.compile(loss='mean_squared_error',optimizer=opt)
	return model

def load_model(parent_dir,model_name):
	#return create_model((70,5))
	#print(os.path.join(parent_dir,model_name))
	return keras.models.load_model(os.path.join(parent_dir,model_name))

# set is either training or validation
def predict_on(model,scaler,setX,setY,price_col):
	prediction = model.predict(setX)
	mse = math.sqrt(mean_squared_error(np.squeeze(setY,axis=2),prediction))

	temp = np.zeros((prediction.shape[0],5))
	temp[:,price_col] = np.squeeze(prediction,axis=1)
	prediction = scaler.inverse_transform(temp)
	return prediction, mse

def extract_month(date):
	temp = date.split("-")
	return temp[1]

def split_data_by_month(data, data_dt, time_step):
	#data_dt['MONTH'] = data_dt['DATE'].str[5:7] # assumes YYYY-MM-DD format
	data_dt['MONTH'] = data_dt['DATE'].apply(extract_month)
	first_month = data_dt.loc[0,'MONTH']
	#print("printing datemonth",data_dt.loc[0,'DATE'],data_dt.loc[0,'MONTH'])
	# count the number of entries that occurred in the first month
	m1_count = len(data_dt.groupby('MONTH').get_group(first_month).index)
	#print("printing # of month 1 entries:",m1_count,time_step)
	if m1_count <= time_step:
		#print("error")
		return None, None
	m1_ind = m1_count - time_step
	return data[:m1_ind], data[m1_ind:]

def construct_prediction(target_fn,model,time_step=70):
	data = pd.read_csv(target_fn,delim_whitespace=True)
	data, scaler, data_dt, price_col = prep_data(data)
	dataX, dataY = make_input_data(data, time_step)
	trainX, testX = split_data_by_month(dataX, data_dt, time_step)
	trainY, testY = split_data_by_month(dataY, data_dt, time_step)
	#print("Displaying input shapes...........................")
	#print(dataX.shape,dataY.shape)
	#print(trainX.shape,testX.shape)
	#print(trainY.shape,testY.shape)
	#model.fit(trainX,trainY,epochs=1,batch_size=32,verbose=1)
	predictTest, _ = predict_on(model,scaler,testX,testY,price_col)

	return predictTest[:,price_col]

###########################################################
######### COMPUTE BEST TIME FROM PREDICTED VALUES #########
###########################################################

# given a list of values, it attempts to find the index of largest value
def opt_stopping(list_vals,stopping=.3678):
	stop_ind = int(len(list_vals)*stopping)
	max_ind = 0

	# in sampling phase, get the index of the largest value
	for i,val in enumerate(list_vals[:stop_ind]):
		#print(val,type(val))
		#print(list_vals,type(list_vals))
		#print(list_vals[max_ind],type(list_vals[max_ind]))
		if list_vals[max_ind] < val:
		  max_ind = i
	# in obtaining phase, return the index of the first value larger than the max
	# val found in the sampling phase
	for i,val in enumerate(list_vals[stop_ind:]):
		if list_vals[max_ind] < val:
		  return i
	return len(list_vals) - 1

def convert_ind_to_dt(ind,valid_days,hrs_per_day=7):
	day_offset  = int(ind/hrs_per_day)
	if day_offset >= len(valid_days): 
		day_offset = len(valid_days) - 1
	day = valid_days[day_offset]

	hour_offset = ind % hrs_per_day
	time = zero_pad(9 + hour_offset) + ":30"

	return day, time

# returns the "best" day and time to sell a certain stock
def predict_sell_time(write_dir,year,stock,model,valid_days,ts=70,stop=.2):
	#target_fn = os.path.join(write_dir,stock + '.csv')
	target_fn = make_output_fn(stock,year,write_dir)
	predict_vals = construct_prediction(target_fn,model,time_step=ts)
	best_ind = opt_stopping(predict_vals,stopping=stop)
	return convert_ind_to_dt(best_ind,valid_days)

###########################################################
############# CONVERT BEST TIME TO VALID TIME #############
###########################################################

# returns a list of subdirectories names (i.e. days)
def get_valid_days(month_dir):
	#print(month_dir)
	for (root, dirnames, filenames) in os.walk(month_dir):
		#print(root)
		#print(dirnames)
		#print(filenames)
		return dirnames
	#print('exiting valid days function ~~~~~~~~~~~~~~~~~~')

# returns the most recent time of a transaction that occurred in streaming
def most_recent_transaction_time(month_dir,stock,day,time):
	df = pd.read_csv(os.path.join(month_dir,day,'streaming.tsv'),delim_whitespace=True)
	#print(df.columns)
	df = df.groupby(['SYMB']).get_group(stock)
	# np.searchsorted(column,target) gets the index of where target would be
	# inserted. subtract one 1 to get the index of the row right before target
	# clip(0) prevents the index from being negative (i.e. trying to get -1 index)
	last_t = df.iloc[(np.searchsorted(df.TIME,time)-1).clip(0)]
	return last_t['TIME']

###########################################################
############# RETRIEVING FUNDAMENTAL ANALYSIS #############
###########################################################

def upper(x):
	return x.upper()

# return a dictionary of stock symbols to [price,DCV,# of stocks to purchase]
def load_fundamental_data(fund_fp,portfolio_size=50):
	df = pd.read_csv(fund_fp,delim_whitespace=True) ## MODIFIED AFTER SUBMISSION
	# sort dataframe by Score
	df.sort_values(by=['Score'],ascending=False,inplace=True)
	# keep only the top X number of stocks
	df = df.head(portfolio_size)
	# drop unnecessary columns
	df = df[['company_name','recent_price','daily_volume']]
	# capitalize all letters in the ticker symbol column
	df.loc[:,'company_name'] = df['company_name'].apply(upper)
	return df

def add_on_stock_cap(df,portfolio_size,cash=1e5,pDCV=1e-2):
	# compute max number of stocks to purchase using 100k/(psize * stock price)
	df['stock_cap_alloc'] = (cash / portfolio_size) / df['recent_price']
	# compute max number of stocks to purchase using 1% of DCV
	df['stock_cap_dcv']   = pDCV * df['daily_volume']
	# compute actual cap
	df['stock_cap'] = df[['stock_cap_alloc','stock_cap_dcv']].apply(min,axis=1)

	# drop columns used to compute stock cap
	#df.drop(['recent_price','daily_volume','stock_cap_alloc','stock_cap_dcv'])
	df = df[['company_name','recent_price','stock_cap']]
	return df

def get_stock_dict_from_fundamental(fund_fp,cash=1e5,portfolio_size=50):
	df = load_fundamental_data(fund_fp,portfolio_size)
	df = add_on_stock_cap(df,portfolio_size, cash)
	# convert dataframe to dict: keys are symbols and values are daily vol
	stock_dict = df.set_index('company_name').T.to_dict('list')
	return stock_dict

###########################################################
################# MAKE TRANSACTION LISTS ##################
###########################################################

def format_transaction(day,time,action,shares,stock):
	#30 15:46 sell 11 shares of EWRM
	return day + " " + time + " " + action + " " + str(shares) + " shares of " + stock

def make_buy_transaction_list(stock_dict,month_dir,buy_time="09:30"):
	#print(month_dir)
	valid_days = get_valid_days(month_dir)
	first_day = valid_days[0]
	buy_list = len(stock_dict.keys()) * [None]
	for i,stock in enumerate(stock_dict.keys()):
		actual_time = most_recent_transaction_time(month_dir,stock,first_day,buy_time)
		buy_list[i] = format_transaction(first_day,actual_time,'buy',stock_dict[stock],stock)
		#print('buy',i,stock)
	return buy_list

def make_sell_transaction_list(stock_dict,year,month_dir,write_dir,model,ts=70,stop=.2):
	valid_days = get_valid_days(month_dir)
	sell_list = len(stock_dict.keys()) * [None]
	for i,stock in enumerate(stock_dict.keys()):
		bday,btime = predict_sell_time(write_dir,year,stock,model,valid_days,ts,stop)
		actual_time = most_recent_transaction_time(month_dir,stock,bday,btime)
		sell_list[i] = format_transaction(bday,actual_time,'sell',stock_dict[stock],stock)
		#print('sell',i,stock)
	return sell_list

###########################################################
###################### MAIN FUNCTION ######################
###########################################################

def write_list_to_file(input_list,target_fn):
	with open(target_fn, mode='wt', encoding='utf-8') as myfile:
		myfile.write('\n'.join(input_list))

def sample_dict():
	d = {}
	d['AAPL'] = 10
	d['MSFT'] = 12
	d['AMZN'] = 14
	return d

def main():
	# optional parameters: portfolio size (50), allocated cash (90k), stopping percent (20%)
	ym1, ym2, fund_fp, write_dir, out_fn = sys.argv[1:]
	
	#pd.options.mode.chained_assignment = None
	#tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
		
	year,m1 = ym1.split('/')
	_   ,m2 = ym2.split('/')
	curr_dir = os.path.realpath('.')
	#print("loading model")	
	#model = load_model('.','model.h5')
	#print("loaded model")
	#return
	# get a dictionary with {key,value} = {stock,# of shares to purchase}
	psize = 50
	stock_dict = get_stock_dict_from_fundamental(os.path.join(write_dir,fund_fp),cash=9e4,portfolio_size=psize)
	#stock_dict = sample_dict()
	
	# convert streaming files into csv files (one csv per stock)
	retrieve_months(curr_dir,year,[m1,m2],write_dir,stock_dict.keys())
	
	# create transaction list
	month_dir = os.path.join(curr_dir,year,m2)
	#print('making buy list')
	buy_list  = make_buy_transaction_list(stock_dict,month_dir)
	#print('loading model')
	model = load_model(curr_dir,'m1_t2x.h5')
	#print('making sell transactions')
	sell_list = make_sell_transaction_list(stock_dict,year,month_dir,write_dir,model,stop=.2)
	transaction_list = buy_list + sell_list
	
	# convert list to .txt file
	write_list_to_file(transaction_list,os.path.join(write_dir,out_fn))
	#print('finished!!')
main()
